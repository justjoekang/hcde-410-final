{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb835c03-1f6b-41ce-a6d6-78202c8e49fa",
   "metadata": {},
   "source": [
    "<h1>Final Project Report</h1>\n",
    "<h2>Measuring TextBlob's Sentiment Analysis through ChatGPT-related Tweets</h2>\n",
    "Joseph Kang<br />  \n",
    "HCDE 410<br />  \n",
    "June 4th, 2023\n",
    "\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "    ChatGPT is a new piece of technology that has increasingly made its way into mainstream media; one would find it very difficult to find someone who's never heard of ChatGPT at our university, for example. I recently did an assignment for a seminar class that made me use ChatGPT to ask questions about various HCD topics and to annotate the results. I was shocked to see how accurate some of the responses to the questions I entered were. While ChatGPT is a very powerful tool, there are potential ramifications or problems it could pose in the future, such as privacy and safety concerns. Therefore, I wanted to see if I could do a project tracking some public sentiment regarding the tool. I hope to learn how people are feeling about ChatGPT as a whole. Are people concerned, fascinated/excited, or maybe downright terrified?<br /> \n",
    "    \n",
    "Throughout this project, I had difficulty with scope and time regarding what I could accomplish, given other classes and responsibilities, so it made it difficult to exactly narrow down what I wanted to. I ended up finding TextBlob, which is a Python library that's primarily used for basic text-based natural language processing. It was simple to use and I felt that it would be really interesting to explore. \n",
    "\n",
    "With the intent of exploring more on TextBlob, I decided to center my project around its validity in context of ChatGPT-related tweets.\n",
    "\n",
    "\n",
    "<h3>Background and Related Work</h3>\n",
    "The dataset I chose to use for this project contains 500 thousand tweets with the word \"ChatGPT\" inside from January to March of 2023. It tracks information regarding the exact date and time of the tweet, the user who tweeted, the message itself, and the number of likes and retweets. The link to the dataset is below: \n",
    "[Dataset](https://www.kaggle.com/datasets/khalidryder777/500k-chatgpt-tweets-jan-mar-2023)\n",
    "The license for this data is CC:0, Public Domain, which means that I can copy, modify, or use their data without their permission.\n",
    "\n",
    "This dataset contains a massive amount of tweets: exceeding a little over five hundred thousand. This makes it really extenstive in terms of the information that it can provide over the given timespan. \n",
    "\n",
    "Through researching related work, I found a research [paper](https://arxiv.org/abs/2212.05856), written by Mubin Ul Haque, Isuru Dharmadasa, Zarrin Tasnim Sworna, Roshan Namal Rajapakse, and Hussain Ahmad. In the paper, they did mixed method research in order to uncover user sentiments regarding Twitter in ChatGPT's early stages. Their findings showed that overwhelmingly, users shared very positive sentiments regarding ChatGPT and its uses. \n",
    "\n",
    "I was informed by this paper by learning some of the steps they took in order to properly cleaning the data, including removing duplicates. I had several questions regarding their choices to remove emojis, to lowercase all of their text, etc, but considering the time alotted for this project, I will not have the chance to make extensive research farther than theirs. \n",
    "\n",
    "TextBlob tracks two different scores for text messages: polarity and subjectivity. Polarity is a measure of the text's sentiment, ranging from -1 to 1, where -1 is extremely negative, and 1 is extremely positive. Subjectivity is a score between 0 and 1 that defines fact or opinion. A score of 0 indicates all opinion and a score of 1 indicates all fact. For this given project, I centered it around polarity as I was concerned about user sentiments. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8f809-f02e-4056-b535-a1d7fc5d8624",
   "metadata": {},
   "source": [
    "<h3>Research Questions</h3>\n",
    "As this project has continued to change over the course of the quarter, so have my intended research questions. <br /> \n",
    "\n",
    "<h5>1. What is the validity of TextBlob's Polarity score? What types of text does TextBlob struggle to score correctly? <br /> \n",
    "2. If TextBlob is working to a high enough degree, within our dataset, how are people's sentiments regarding ChatGPT changing, if at all?</h5>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c63e2-1d4b-43d0-8b31-50b98d7138c8",
   "metadata": {},
   "source": [
    "<h3>Methodology</h3>\n",
    "    There were many difficulties when attempting to work on this project that I had not thought of when originally planning it. The first and foremost concern I ran into was with text pre-processing and data cleaning; with a dataset as big as 50,000 tweets, there was bound to be some mistakes and null values that could possibly make it difficult to run my code. In addition, processing tools like TextBlob require some pre-processing to some degree because of punctuation that might ruin the integrity of .csv files.\n",
    "    \n",
    "\n",
    "The second major issue regarding this project was the very massive dataset; while a large dataset is nice to conduct extensive research, I found that the my code would take far too much time to run and that made this project much harder than I originally imagined. I will discuss in more detail how I attempted to alleviate some of these issues, but they are problems nonetheless. \n",
    "\n",
    "<h5> Step 1: Setup </h5>    \n",
    "The below code initializes the csv, textblob and random libraries I used for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "624d0291-0e7c-4957-81f3-49cf056d5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a873a-c94c-4540-bf24-ae6eddd89e09",
   "metadata": {},
   "source": [
    "<h5>Step 2: Functions</h5> The next important step was to create a function that allowed me to append a TextBlob value column onto my dataset. The below function takes in a given row of our dataset, calculates the polarity value using TextBlob, and appends it onto the end of the row, effectively creating a new column for the TextBlob value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c2c346-776d-441d-b81f-52d330db0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to add a textblob column\n",
    "def add_textblob_column(tweet):\n",
    "    content = tweet[2]  # assumes the tweet content is in the third column\n",
    "    blob = TextBlob(content)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    return tweet + [polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d64ac-b3ee-402a-a687-a9a57280eebc",
   "metadata": {},
   "source": [
    "In order to use my dataset with my given resources, I realized that I couldn't manage to run code on a dataset this large given my constraints. I ended up taking a random sample of 2000 tweets of the given 50,000 in the hopes that it would drastically reduce the amount of time and strain it was putting on my laptop. \n",
    "\n",
    "My input file (from the Kaggle dataset), 'tweet.csv' and my output file (which I just named) to 'random_sample.csv' would be the variables I need for my next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6250825c-418e-492b-87bc-976c1b594708",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'tweets.csv'\n",
    "output_file = 'random_sample.csv'\n",
    "sample_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37054b2f-1e2e-4899-91d6-ece10472b586",
   "metadata": {},
   "source": [
    "The below function takes in a input file, the name of an output file, and a given sample size in order to take a random sample. The random sample variable can be changed above for smaller or greater effects. I chose 2000 for this project because I felt that it was a high enough number for me to create somewhat effective data visualizations, but also not strain my laptop or take up too much time running code.\n",
    "\n",
    "I used ChatGPT (ironic) to help debug some of this code as I was having errors with it early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57727110-a667-4110-982a-055d42077c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a random sample of tweets\n",
    "def take_random_sample(input_file, output_file, sample_size):\n",
    "    with open(input_file, 'r') as csv_input, open(output_file, 'w', newline='') as csv_output:\n",
    "        reader = csv.reader(csv_input)\n",
    "        writer = csv.writer(csv_output)\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "        lines = list(reader)\n",
    "        random_lines = random.sample(range(len(lines)), sample_size)\n",
    "        for line_number in random_lines:\n",
    "            writer.writerow(lines[line_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1cd611b-3d7a-4df7-ba96-2db2d4a07b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_random_sample(input_file, output_file, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad7827-1f74-4b30-90f7-ca3137780847",
   "metadata": {},
   "source": [
    "Running the above code would now give me random sample of 2000 tweets from my dataset, which would allow me to create data visualizations using Tableau to see if any trends could be found. These data visualizations are provided in the project folder.\n",
    "\n",
    "<h5>Step 3: Execution</h5>\n",
    "The below code is the last step of our process, which uses the TextBlob value appending function from the beginning of Step 2 on the random sample dataset in order to get us a dataset of 2000 tweets with appended TextBlob values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3662264-3080-432b-985e-afe5647e0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_input_file = 'random_sample.csv'\n",
    "r_output_file = 'random_textblob.csv'\n",
    "\n",
    "with open(r_input_file, 'r') as csv_input, open(r_output_file, 'w', newline='') as csv_output:\n",
    "    reader = csv.reader(csv_input)\n",
    "    writer = csv.writer(csv_output)\n",
    "    \n",
    "    header = next(reader)\n",
    "    header_with_textblob = header + ['TextBlob']\n",
    "    writer.writerow(header_with_textblob)\n",
    "    \n",
    "    for tweet in reader:\n",
    "        if len(tweet) > 0:\n",
    "            tweet[2] = tweet[2].replace(',','') # this line was necessary because the commas in the tweet text were \n",
    "            # damaging the integrity of the file, the content of the tweets were everywhere which made it extremely difficult\n",
    "            # to work with. \n",
    "            tweet_with_textblob = add_textblob_column(tweet)\n",
    "            writer.writerow(tweet_with_textblob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63684341-cc1e-4ce8-9cb4-86fbc12a5f17",
   "metadata": {},
   "source": [
    "<h3>Findings, Limitations and Implications</h3>\n",
    "After cleaning the data, I created data visualizations on Tableau to see if I could find any trends regarding TextBlob. To start, I plotted the TextBlob values as a distribution, and I found that overwhelmingly, almost a majority of tweets were labelled as polarity 0, meaning completely neutral. \n",
    "\n",
    "This is very difficult to believe, as it's hard to make any message, let alone messages on the internet, completely neutral. Most tweets must have some form of emotion to them, negative or positive. One possible explanation for this is the large amount of bots on Twitter. Just by parsing through the data by hand, I found loads of tweets that were from advertising bots on Twitter, usually providing some short text and a link to some third party advertisment regarding ChatGPT. This has massive implications for the validity of my project and is a massive limitation as well, and there needs to be some way to parse through my dataset and eliminate these bots. I cannot find conclusive evidence regarding public sentiment on ChatGPT if the majority of my dataset comes from automated bots, not humans. \n",
    "\n",
    "After sorting my .csv file and parsing through it by hand, I found numerous tweets that I personally felt were not adequately rated. I found several tweets saying that ChatGPT was 'insane' marked as -1 polarity, meaning that TextBlob felt that the comment was extremely toxic. I found this really strange and wasn't sure exactly why, perhaps the system saw the word 'insane' and defaulted to it being used in toxic situations? I also found examples of messages where people were cursing out ChatGPT, but using special symbols helped them avoid it being detected as toxic. It appears that TextBlob has limitations in what it can or can't detect, and special symbols might be an area where it can improve. \n",
    "\n",
    "One other major limitation regarding this project process is my removal of commas in pre-processing. In order to run my code and visualize my data, I was forced to remove commas from all texts because it was disorganizing my columns because a csv file is separated by commas. Punctuation is a very important part of communication, and especially sentences without commas can have drastically different meanings depending on where those commas were placed. Removing punctuation, of any form, is a major consideration when doing natural-language processing work. While it may make your work more convenient, it is also very damaging to the integrity of your project. The paper I read in reference for related work also pre-processed their code, where I believe they removed all punctuation. \n",
    "\n",
    "I created a data visualization plotting the average polarity rating for tweets for that given day over the course of my dataset, but that produced a trend that wasn't very conclusive, especially since we know that both TextBlob, Twttier bots, and my study methods have major limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480ee25-23b8-4f6d-b1c4-94c6c393cfbe",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "I recognize that this project was very inconclusive in regards to actual findings, but I believe that it helped give lots of valuable insight into what it might take in order to do a similar project at a much higher scale. \n",
    "\n",
    "Some major considerations are:\n",
    "\n",
    "<h6>1. Large Sample Sizes</h6>\n",
    "Large sample sizes are important for coming up with overarching trends: too small a sample size have large amounts of variation. At the same time though, it's important that you have both the technical skills and the technical resources in order to run code on data this large. \n",
    "<h6>2. Twitter Bots</h6>\n",
    "These days, Twitter has an overwhelming amount of automated bots on Twitter constantly spamming links, hashtags, and other messages. These may negatively affect your outcomes, and therefore finding a way to minimize the effects of the bots, or even better, remove them all completely, would be very much worth exploring.\n",
    "<h6>3. Natural Language Pre-processing </h6>\n",
    "When performing natural language processing tasks, it's not shocking to know that your data needs to be pre-processed and cleaned. Many papers regarding natural language processing that I've read end up removign punctuation, putting all text in lowercase, removing links, emojis, etc. While this may make your technical job easier, it has major implications on the validity of your conclusions. Emojis, capital letters, and punctuation all have massive implications on the meaning and emotion behind a text message. There is too much contextual evidence that can be easily removed from simple convenience pre-processing steps. A better way to process this data is a must to create a better study.\n",
    "<h6>4. The Validity of NLP Models</h6>\n",
    "There is a lot different models out there on the internet that can help you with processing text and attempting to classify it by emotion. Some of them are better than others. While no model training was done in this project, training your own model using hand-coded data might be a good consideration. I noticed that TextBlob had various difficulties taking in text and classifying it properly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1699f-697c-4499-b674-763b9547f76e",
   "metadata": {},
   "source": [
    "I personally am very fascinated by natural-language processing models and how they can be used in the future to help monitor social media, videogames, and other chat platforms to make them more accessible and safer places. However, my work in this project alone should be a small, but significant telling that natural-language processing tools are FAR more difficult than people might imagine and that they require a deep understanding of contextual and systematic limitations in regards to your process and the resources you use (including model and dataset).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
